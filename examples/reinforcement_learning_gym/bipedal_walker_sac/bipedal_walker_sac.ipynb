{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://lab.mlpack.org/v2/gh/mlpack/examples/master?urlpath=lab%2Ftree%2Freinforcement_learning_gym%2Fbipedal_walker_sac%2Fbipedal_walker_sac.ipynb)\n",
    "\n",
    "You can easily run this notebook at https://lab.mlpack.org/\n",
    "\n",
    "Here, we train a [Soft Actor-Critic](https://arxiv.org/abs/1801.01290) agent to get high scores for the [Bipedal Walker](https://gym.openai.com/envs/BipedalWalker-v2/) environment. \n",
    "\n",
    "We make the agent train and test on OpenAI Gym toolkit's GUI interface provided through a distributed infrastructure (TCP API). More details can be found [here](https://github.com/zoq/gym_tcp_api).\n",
    "\n",
    "A video of the trained agent can be seen in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including necessary libraries and namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/core.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/sac.hpp>\n",
    "#include <mlpack/methods/ann/loss_functions/empty_loss.hpp>\n",
    "#include <mlpack/methods/ann/init_rules/gaussian_init.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/env_type.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/training_config.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to run the agent on gym's environment (provided externally) for testing.\n",
    "#include <gym/environment.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to generate and display a video of the trained agent.\n",
    "#include \"xwidgets/ximage.hpp\"\n",
    "#include \"xwidgets/xvideo.hpp\"\n",
    "#include \"xwidgets/xaudio.hpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace ens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::rl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the state and action space.\n",
    "ContinuousActionEnv::State::dimension = 24;\n",
    "ContinuousActionEnv::Action::size = 4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the actor and critic networks.\n",
    "FFN<EmptyLoss<>, GaussianInitialization>\n",
    "    policyNetwork(EmptyLoss<>(), GaussianInitialization(0, 0.01));\n",
    "policyNetwork.Add(new Linear<>(ContinuousActionEnv::State::dimension, 128));\n",
    "policyNetwork.Add(new ReLULayer<>());\n",
    "policyNetwork.Add(new Linear<>(128, 128));\n",
    "policyNetwork.Add(new ReLULayer<>());\n",
    "policyNetwork.Add(new Linear<>(128, ContinuousActionEnv::Action::size));\n",
    "policyNetwork.Add(new TanHLayer<>());\n",
    "policyNetwork.ResetParameters();\n",
    "\n",
    "FFN<EmptyLoss<>, GaussianInitialization>\n",
    "    qNetwork(EmptyLoss<>(), GaussianInitialization(0, 0.01));\n",
    "qNetwork.Add(new Linear<>(ContinuousActionEnv::State::dimension + ContinuousActionEnv::Action::size, 128));\n",
    "qNetwork.Add(new ReLULayer<>());\n",
    "qNetwork.Add(new Linear<>(128, 128));\n",
    "qNetwork.Add(new ReLULayer<>());\n",
    "qNetwork.Add(new Linear<>(128, 1));\n",
    "qNetwork.ResetParameters();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the replay method.\n",
    "RandomReplay<ContinuousActionEnv> replayMethod(32, 10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up training configurations.\n",
    "TrainingConfig config;\n",
    "config.ExplorationSteps() = 3200;\n",
    "config.TargetNetworkSyncInterval() = 1;\n",
    "config.UpdateInterval() = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we load a pretrained model by manually assigning values to the parameters of the network, after loading the parameters from their respective files `sac_q.txt` and `sac_policy.txt`.\n",
    "\n",
    "The model was trained for 620 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "arma::mat temp;\n",
    "data::Load(\"sac_q.txt\", temp);\n",
    "qNetwork.Parameters() = temp.t();\n",
    "data::Load(\"sac_policy.txt\", temp);\n",
    "policyNetwork.Parameters() = temp.t();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can train the model from scratch by running the following: \n",
    "```c++\n",
    "// Set up Soft actor-critic agent.\n",
    "SAC<ContinuousActionEnv, decltype(qNetwork), decltype(policyNetwork), AdamUpdate>\n",
    "    agent(config, qNetwork, policyNetwork, replayMethod);\n",
    "\n",
    "const std::string environment = \"BipedalWalker-v3\";\n",
    "const std::string host = \"127.0.0.1\";\n",
    "const std::string port = \"4040\";\n",
    "\n",
    "Environment env(host, port, environment);\n",
    "\n",
    "std::vector<double> returnList;\n",
    "size_t episodes = 0;\n",
    "bool converged = true;\n",
    "size_t consecutiveEpisodesTest = 50;\n",
    "while (true)\n",
    "{\n",
    "    double episodeReturn = 0;\n",
    "    env.reset();\n",
    "    size_t steps = 0;\n",
    "    do\n",
    "    {\n",
    "        agent.State().Data() = env.observation;\n",
    "        agent.SelectAction();\n",
    "        arma::mat action = {agent.Action().action};\n",
    "\n",
    "        env.step(action);\n",
    "        ContinuousActionEnv::State nextState;\n",
    "        nextState.Data() = env.observation;\n",
    "\n",
    "        replayMethod.Store(agent.State(), agent.Action(), env.reward, nextState, env.done, 0.99);\n",
    "        episodeReturn += env.reward;\n",
    "        agent.TotalSteps()++;\n",
    "        steps++;\n",
    "        if (agent.Deterministic() || agent.TotalSteps() < config.ExplorationSteps())\n",
    "            continue;\n",
    "        for (size_t i = 0; i < config.UpdateInterval(); i++)\n",
    "            agent.Update();\n",
    "    } while (!env.done);\n",
    "    returnList.push_back(episodeReturn);\n",
    "    episodes += 1;\n",
    "\n",
    "    if (returnList.size() > consecutiveEpisodesTest)\n",
    "        returnList.erase(returnList.begin());\n",
    "\n",
    "    double averageReturn = std::accumulate(returnList.begin(),\n",
    "                                           returnList.end(), 0.0) /\n",
    "                           returnList.size();\n",
    "\n",
    "    std::cout << \"Average return in last \" << returnList.size()\n",
    "              << \" consecutive episodes: \" << averageReturn\n",
    "              << \" steps: \" << steps\n",
    "              << \" Episode return: \" << episodeReturn << std::endl;\n",
    "\n",
    "    if (episodes % 10 == 0)\n",
    "    {\n",
    "        data::Save(\"./\" + std::to_string(episodes) + \"qNetwork.xml\", \"episode_\" + std::to_string(episodes), qNetwork);\n",
    "        data::Save(\"./\" + std::to_string(episodes) + \"policyNetwork.xml\", \"episode_\" + std::to_string(episodes), policyNetwork);\n",
    "    }\n",
    "    if (averageReturn > -50)\n",
    "        break;\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained agent\n",
    "\n",
    "It is so amazing to see how just a matrix of numbers, operated in a certain fashion, is able to develop a walking gait. \n",
    "\n",
    "Thats the beauty of Artificial Neural Networks! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 1092\t Total reward: 255.508\n"
     ]
    }
   ],
   "source": [
    "// Set up Soft actor-critic agent.\n",
    "SAC<ContinuousActionEnv, decltype(qNetwork), decltype(policyNetwork), AdamUpdate>\n",
    "    agent(config, qNetwork, policyNetwork, replayMethod);\n",
    "\n",
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"BipedalWalker-v3\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {agent.Action().action};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "std::cout << url;\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
